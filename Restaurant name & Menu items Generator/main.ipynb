{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1424d08-522c-49db-b8e4-2debe726a280",
   "metadata": {},
   "outputs": [],
   "source": [
    "from api_keys import hf_api\n",
    "import os\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"]=hf_api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1d162218-b466-4b8c-98b6-87a55428c8fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "\n",
      "“Spice Haven”\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "model = InferenceClient(model=\"mistralai/Mistral-7B-Instruct-v0.1\")\n",
    "question = \"I would love to open an Indian Restaurant. Suggest a fancy and attractive name for it.Do not include any extra text, just a single name\"\n",
    "response = client.text_generation(question, max_new_tokens=10, temperature=0.7)\n",
    "print(response.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42ba1080-a6ba-4888-bc50-adde87998c0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Belladonna\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "from api_keys import gemini_api\n",
    "\n",
    "# Set your Gemini API key\n",
    "os.environ[\"GOOGLE_API_KEY\"] = gemini_api\n",
    "\n",
    "# Load Gemini model\n",
    "llm = GoogleGenerativeAI(model=\"gemini-1.5-pro\")\n",
    "\n",
    "# Ask for a restaurant name\n",
    "question = \"I would love to open an italian Restaurant. Suggest a fancy and attractive name for it.Do not include any extra text, just a single name\"\n",
    "name = llm.invoke(question)\n",
    "\n",
    "# Print only the name\n",
    "print(name.strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1d7144cb-e815-4568-a5b5-90e2401839d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I would love to open an Mexican Restaurant. Suggest a fancy and attractive name for it, just a name'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "prompt_template_name = PromptTemplate(\n",
    "    input_variables = ['cusine'],\n",
    "    template = \"I would love to open an {cuisine} Restaurant. Suggest a fancy and attractive name for it, just a name\"\n",
    ")\n",
    "prompt_template_name.format(cuisine = \"Mexican\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "80f44aa5-3c7d-4d39-bb0a-162d6846faa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Saffron & Silk'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "chain = LLMChain(llm = llm, prompt = prompt_template_name)\n",
    "chain.run(\"Indian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "079d0584-a31d-45cb-adbd-e72f8cc3c290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Sequential Chain \n",
    "\n",
    "llm_model = GoogleGenerativeAI(model = \"gemini-1.5-pro\", temperature=0.7)\n",
    "prompt_template_name = PromptTemplate(\n",
    "    input_variables=['cuisine'],\n",
    "    template = \"I would love to open an {cuisine} Restaurant. Suggest a fancy and attractive name for it, just a name\"\n",
    ")\n",
    "name_chain = LLMChain(llm = llm_model, prompt = prompt_template_name)\n",
    "\n",
    "prompt_template_items = PromptTemplate(\n",
    "    input_variables = ['restaurant_name'],\n",
    "    template = \"Suggest some menu items for {restaurant_name}. Return it as comma seperated values\"\n",
    ")\n",
    "food_items_chain = LLMChain(llm = llm_model, prompt = prompt_template_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "df7495a7-3d66-470e-a9ce-643fe1dd71b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dal Makhani, Butter Chicken, Palak Paneer, Rogan Josh, Tandoori Chicken, Samosas, Vegetable Biryani, Lamb Korma, Naan, Garlic Naan, Aloo Gobi, Chicken Tikka Masala, Gulab Jamun, Mango Lassi, Masala Chai\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import SimpleSequentialChain\n",
    "\n",
    "chain = SimpleSequentialChain(chains = [name_chain,food_items_chain])\n",
    "response=chain.run(\"India\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3b8739c1-6401-4fad-8186-a21b7354f42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, the Sequential Chain is only printing one output\n",
    "# i.e, it is taking the restaurant name as input and printing the menu items \n",
    "# it is not printing the initial restaurant name\n",
    "# so, to print that, we'll use Sequential chain, not the SimpleSequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5be82b6c-ed2f-475c-b5b3-a1087c198c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequential Chain\n",
    "\n",
    "llm_model = GoogleGenerativeAI(model = \"gemini-1.5-pro\", temperature = 0.7)\n",
    "prompt_template_name = PromptTemplate(\n",
    "    input_variables = ['cuisine'],\n",
    "    template = \"I would love to open an {cuisine} Restaurant. Suggest a fancy and attractive name for it, just a name\"\n",
    ")\n",
    "name_chain = LLMChain(llm = llm_model, prompt=prompt_template_name, output_key = \"restaurant_name\")\n",
    "\n",
    "prompt_template_items = PromptTemplate(\n",
    "    input_variables=['restaurant_name'],\n",
    "    template = \"Suggest some menu items for {restaurant_name}. Return it as comma seperated values\"\n",
    ")\n",
    "items_chain=LLMChain(llm = llm_model,prompt=prompt_template_items,output_key=\"menu_items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dfa31d70-e9d9-4f16-818a-f9d6fba81a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Maniram\\AppData\\Local\\Temp\\ipykernel_3196\\193685649.py:7: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  chain({'cuisine':'Arabic'})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'cuisine': 'Arabic',\n",
       " 'restaurant_name': 'Alhambra',\n",
       " 'menu_items': 'Tapas: Patatas Bravas, Gambas al Ajillo, Tortilla Española, Croquetas de Jamón, Espinacas con Garbanzos, Boquerones en Vinagre, Pimientos de Padrón.\\n\\nMains: Paella Valenciana, Paella de Marisco, Fideuà, Cochinillo Asado, Rabo de Toro, Solomillo al Whisky, Bacalao a la Vizcaína.\\n\\nDesserts: Crema Catalana, Tarta de Santiago, Churros con Chocolate, Flan de Huevo, Arroz con Leche.\\n\\nDrinks: Sangria, Tinto de Verano, Cerveza, Vino Tinto, Vino Blanco.\\n\\n\\nSo, as a comma separated list:  Patatas Bravas, Gambas al Ajillo, Tortilla Española, Croquetas de Jamón, Espinacas con Garbanzos, Boquerones en Vinagre, Pimientos de Padrón, Paella Valenciana, Paella de Marisco, Fideuà, Cochinillo Asado, Rabo de Toro, Solomillo al Whisky, Bacalao a la Vizcaína, Crema Catalana, Tarta de Santiago, Churros con Chocolate, Flan de Huevo, Arroz con Leche, Sangria, Tinto de Verano, Cerveza, Vino Tinto, Vino Blanco'}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import SequentialChain\n",
    "chain = SequentialChain(\n",
    "    chains=[name_chain,items_chain],\n",
    "    input_variables = ['cuisine'],\n",
    "    output_variables=['restaurant_name','menu_items']\n",
    ")\n",
    "chain({'cuisine':'Arabic'})\n",
    "#Here, we don't use the .run fuction, we directly pass the argument, since, there could be any possible cuisines, we use a disctionary for arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6f0a55-ffb8-4ea1-9a3e-d8b36ef3f16c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
